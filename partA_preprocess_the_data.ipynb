{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "058575d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_project_root = 'lab_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c980c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
      "Collecting soxr>=0.3.2\n",
      "  Downloading soxr-1.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.3/242.3 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.2.1)\n",
      "Collecting soundfile>=0.12.1\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting audioread>=2.1.9\n",
      "  Downloading audioread-3.1.0-py3-none-any.whl (23 kB)\n",
      "Collecting msgpack>=1.0\n",
      "  Downloading msgpack-1.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (406 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.1/406.1 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pooch>=1.1\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.15.0)\n",
      "Collecting numba>=0.51.0\n",
      "  Downloading numba-0.62.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.1)\n",
      "Collecting lazy_loader>=0.1\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0\n",
      "  Downloading llvmlite-0.45.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.10.5)\n",
      "Installing collected packages: soxr, msgpack, llvmlite, lazy_loader, audioread, soundfile, pooch, numba, librosa\n",
      "Successfully installed audioread-3.1.0 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.45.1 msgpack-1.1.2 numba-0.62.1 pooch-1.8.2 soundfile-0.13.1 soxr-1.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d63bc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ffe6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path(your_project_root).expanduser()\n",
    "data_root = root_path / \"data\"\n",
    "full_data_root = data_root / \"full\"\n",
    "sc_version = \"speech_commands_v0.02\"\n",
    "sc_url = \"https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd080e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = [\n",
    "    \"yes\", \"no\", \"up\", \"down\", \"left\",\n",
    "    \"right\", \"on\", \"off\", \"stop\", \"go\",\n",
    "    \"silence\", \"unknown\",\n",
    "]\n",
    "TARGET_WORDS = LABEL_NAMES[:10]   \n",
    "SILENCE_LABEL = \"silence\"\n",
    "UNKNOWN_LABEL = \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db2f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 16000         # sampling rate\n",
    "DURATION = 1.0     # duration\n",
    "N_MFCC = 49        # MFCC dimension\n",
    "N_FRAMES = 10      # time frames (1, 10, 49)\n",
    "\n",
    "MAX_UNKNOWN_PER_SPLIT = 3000\n",
    "N_UNKNOWN_TRAIN = MAX_UNKNOWN_PER_SPLIT\n",
    "N_UNKNOWN_VAL = 400\n",
    "N_UNKNOWN_TEST = 400\n",
    "\n",
    "N_SILENCE_TRAIN = 3000\n",
    "N_SILENCE_VAL = 400\n",
    "N_SILENCE_TEST = 400\n",
    "\n",
    "OUT_NPZ = data_root / \"kws_12cls_mfcc_10x49.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f34d7cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] extracting to：lab_new/data/full\n",
      "[INFO] extraction completed：lab_new/data/speech_commands_v0.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('lab_new/data/speech_commands_v0.02')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_and_extract_speech_commands():\n",
    "    data_root.mkdir(parents=True, exist_ok=True)\n",
    "    tar_path = data_root / f\"{sc_version}.tar.gz\"\n",
    "    sc_root = data_root / sc_version\n",
    "\n",
    "    if sc_root.exists():\n",
    "        print(f\"[INFO] data tar exists：{sc_root}\")\n",
    "        return sc_root\n",
    "\n",
    "    if not tar_path.exists():\n",
    "        print(f\"[INFO] downloading Speech Commands v0.02 tar to：{tar_path}\")\n",
    "        urllib.request.urlretrieve(sc_url, tar_path)\n",
    "        print(f\"[INFO] download completed.\")\n",
    "\n",
    "    print(f\"[INFO] extracting to：{full_data_root}\")\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=full_data_root)\n",
    "    print(f\"[INFO] extraction completed：{sc_root}\")\n",
    "\n",
    "    return sc_root\n",
    "\n",
    "download_and_extract_speech_commands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6369bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list_file(list_path: Path):\n",
    "    s = set()\n",
    "    with open(list_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                s.add(line)\n",
    "    return s\n",
    "\n",
    "\n",
    "def extract_mfcc_10x49_from_array(y, sr=SR, duration=DURATION,\n",
    "                                  n_mfcc=N_MFCC, n_frames=N_FRAMES):\n",
    "    desired_len = int(sr * duration)\n",
    "    if len(y) < desired_len:\n",
    "        y = np.pad(y, (0, desired_len - len(y)))\n",
    "    else:\n",
    "        y = y[:desired_len]\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_fft=1024,\n",
    "        hop_length=160\n",
    "    )  # shape: (49, T)\n",
    "\n",
    "    T = mfcc.shape[1]\n",
    "    if T < n_frames:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, n_frames - T)), mode=\"edge\")\n",
    "        T = mfcc.shape[1]\n",
    "\n",
    "    idx = np.linspace(0, T - 1, n_frames).astype(int)\n",
    "    feat = mfcc[:, idx]          # (49, 10)\n",
    "    feat = feat.T.astype(np.float32)  # (10, 49)\n",
    "\n",
    "    feat = feat[np.newaxis, ...]\n",
    "    return feat\n",
    "\n",
    "\n",
    "def extract_mfcc_10x49_from_file(wav_path: Path):\n",
    "    y, sr = librosa.load(wav_path, sr=SR)\n",
    "    return extract_mfcc_10x49_from_array(y, sr=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a21f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] label to index mapping:\n",
      "       yes -> 0\n",
      "        no -> 1\n",
      "        up -> 2\n",
      "      down -> 3\n",
      "      left -> 4\n",
      "     right -> 5\n",
      "        on -> 6\n",
      "       off -> 7\n",
      "      stop -> 8\n",
      "        go -> 9\n",
      "   silence -> 10\n",
      "   unknown -> 11\n"
     ]
    }
   ],
   "source": [
    "val_list = read_list_file(full_data_root / \"validation_list.txt\")\n",
    "test_list = read_list_file(full_data_root / \"testing_list.txt\")\n",
    "\n",
    "\n",
    "label_to_idx = {name: i for i, name in enumerate(LABEL_NAMES)}\n",
    "print(\"[INFO] label to index mapping:\")\n",
    "for k, v in label_to_idx.items():\n",
    "    print(f\"  {k:>8s} -> {v}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98ef9d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "X_val,   y_val   = [], []\n",
    "X_test,  y_test  = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b3750e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] dealing with target words ...\n",
      "[INFO] yes: 4044 samples found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  yes: 100%|███████████████████████████████| 4044/4044 [00:27<00:00, 148.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] no: 3941 samples found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   no: 100%|███████████████████████████████| 3941/3941 [00:16<00:00, 236.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] up: 3723 samples found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   up: 100%|███████████████████████████████| 3723/3723 [00:14<00:00, 263.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] down: 3917 samples found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " down: 100%|███████████████████████████████| 3917/3917 [00:17<00:00, 218.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] left: 3801 samples found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " left: 100%|███████████████████████████████| 3801/3801 [00:16<00:00, 228.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] right: 3778 samples found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "right: 100%|███████████████████████████████| 3778/3778 [00:16<00:00, 235.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] on: 3845 samples found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   on: 100%|███████████████████████████████| 3845/3845 [00:17<00:00, 220.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] off: 3745 samples found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  off: 100%|███████████████████████████████| 3745/3745 [00:16<00:00, 220.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] stop: 3872 samples found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " stop: 100%|███████████████████████████████| 3872/3872 [00:17<00:00, 225.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] go: 3880 samples found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   go: 100%|███████████████████████████████| 3880/3880 [00:17<00:00, 224.64it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[INFO] dealing with target words ...\")\n",
    "for word in TARGET_WORDS:\n",
    "    word_dir = full_data_root / word\n",
    "    if not word_dir.exists():\n",
    "        print(f\"[WARN] path does not exist, skipping: {word_dir}\")\n",
    "        continue\n",
    "\n",
    "    wav_files = sorted(word_dir.glob(\"*.wav\"))\n",
    "    print(f\"[INFO] {word}: {len(wav_files)} samples found\")\n",
    "\n",
    "    for wav_path in tqdm(wav_files, desc=f\"{word:>5s}\", ncols=80):\n",
    "        rel_path = wav_path.relative_to(full_data_root).as_posix()\n",
    "\n",
    "        if rel_path in val_list:\n",
    "            subset = \"val\"\n",
    "        elif rel_path in test_list:\n",
    "            subset = \"test\"\n",
    "        else:\n",
    "            subset = \"train\"\n",
    "\n",
    "        feat = extract_mfcc_10x49_from_file(wav_path)\n",
    "        label_idx = label_to_idx[word]\n",
    "\n",
    "        if subset == \"train\":\n",
    "            X_train.append(feat)\n",
    "            y_train.append(label_idx)\n",
    "        elif subset == \"val\":\n",
    "            X_val.append(feat)\n",
    "            y_val.append(label_idx)\n",
    "        else:\n",
    "            X_test.append(feat)\n",
    "            y_test.append(label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4789119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] gathering unknown class samples ...\n",
      "[INFO] unknown ori num: train=54074, val=6278, test=6931\n",
      "[INFO] unknown sampled: train=3000, val=400, test=400\n",
      "[INFO] extracting unknown train features, total 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unk-train: 100%|███████████████████████████| 3000/3000 [00:13<00:00, 229.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] extracting unknown val features, total 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unk-val: 100%|███████████████████████████████| 400/400 [00:01<00:00, 201.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] extracting unknown test features, total 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unk-test: 100%|██████████████████████████████| 400/400 [00:03<00:00, 126.49it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[INFO] gathering unknown class samples ...\")\n",
    "all_dirs = [d for d in full_data_root.iterdir() if d.is_dir()]\n",
    "unknown_dirs = [\n",
    "    d for d in all_dirs\n",
    "    if d.name not in TARGET_WORDS\n",
    "    and d.name not in [\"_background_noise_\"]\n",
    "    and not d.name.startswith(\".\")\n",
    "]\n",
    "\n",
    "unknown_train_files, unknown_val_files, unknown_test_files = [], [], []\n",
    "for d in unknown_dirs:\n",
    "    wav_files = sorted(d.glob(\"*.wav\"))\n",
    "    for wav_path in wav_files:\n",
    "        rel_path = wav_path.relative_to(full_data_root).as_posix()\n",
    "        if rel_path in val_list:\n",
    "            unknown_val_files.append(wav_path)\n",
    "        elif rel_path in test_list:\n",
    "            unknown_test_files.append(wav_path)\n",
    "        else:\n",
    "            unknown_train_files.append(wav_path)\n",
    "\n",
    "print(f\"[INFO] unknown ori num: train={len(unknown_train_files)}, \"\n",
    "        f\"val={len(unknown_val_files)}, test={len(unknown_test_files)}\")\n",
    "\n",
    "def sample_files(file_list, max_n):\n",
    "    if len(file_list) > max_n:\n",
    "        return random.sample(file_list, max_n)\n",
    "    return file_list\n",
    "\n",
    "unknown_train_files = sample_files(unknown_train_files, N_UNKNOWN_TRAIN)\n",
    "unknown_val_files   = sample_files(unknown_val_files,   N_UNKNOWN_VAL)\n",
    "unknown_test_files  = sample_files(unknown_test_files,  N_UNKNOWN_TEST)\n",
    "\n",
    "print(f\"[INFO] unknown sampled: train={len(unknown_train_files)}, \"\n",
    "        f\"val={len(unknown_val_files)}, test={len(unknown_test_files)}\")\n",
    "\n",
    "unk_idx = label_to_idx[UNKNOWN_LABEL]\n",
    "\n",
    "for split_name, file_list in [\n",
    "    (\"train\", unknown_train_files),\n",
    "    (\"val\",   unknown_val_files),\n",
    "    (\"test\",  unknown_test_files),\n",
    "]:\n",
    "    print(f\"[INFO] extracting unknown {split_name} features, total {len(file_list)}\")\n",
    "    for wav_path in tqdm(file_list, desc=f\"unk-{split_name}\", ncols=80):\n",
    "        feat = extract_mfcc_10x49_from_file(wav_path)\n",
    "        if split_name == \"train\":\n",
    "            X_train.append(feat)\n",
    "            y_train.append(unk_idx)\n",
    "        elif split_name == \"val\":\n",
    "            X_val.append(feat)\n",
    "            y_val.append(unk_idx)\n",
    "        else:\n",
    "            X_test.append(feat)\n",
    "            y_test.append(unk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "162bb3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] generating silence samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sil-train: 100%|███████████████████████████| 3000/3000 [00:13<00:00, 223.53it/s]\n",
      "sil-val: 100%|███████████████████████████████| 400/400 [00:01<00:00, 209.94it/s]\n",
      "sil-test: 100%|██████████████████████████████| 400/400 [00:01<00:00, 261.81it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[INFO] generating silence samples ...\")\n",
    "noise_dir = full_data_root / \"_background_noise_\"\n",
    "noise_files = sorted(noise_dir.glob(\"*.wav\"))\n",
    "if not noise_files:\n",
    "    print(\"[WARN] not found _background_noise_/*.wav，silence class will be empty\")\n",
    "    N_sil_train = N_sil_val = N_sil_test = 0\n",
    "else:\n",
    "    noise_clips = []\n",
    "    for nf in noise_files:\n",
    "        y, sr = librosa.load(nf, sr=SR)\n",
    "        noise_clips.append(y)\n",
    "\n",
    "    def gen_silence_samples(n_samples, desc):\n",
    "        feats = []\n",
    "        desired_len = int(SR * DURATION)\n",
    "        for _ in tqdm(range(n_samples), desc=desc, ncols=80):\n",
    "            y = random.choice(noise_clips)\n",
    "            if len(y) > desired_len:\n",
    "                start = random.randint(0, len(y) - desired_len)\n",
    "                y_seg = y[start:start + desired_len]\n",
    "            else:\n",
    "                y_seg = np.pad(y, (0, desired_len - len(y)))\n",
    "            feat = extract_mfcc_10x49_from_array(y_seg, sr=SR)\n",
    "            feats.append(feat)\n",
    "        return feats\n",
    "\n",
    "    sil_idx = label_to_idx[SILENCE_LABEL]\n",
    "\n",
    "    sil_train = gen_silence_samples(N_SILENCE_TRAIN, \"sil-train\")\n",
    "    sil_val   = gen_silence_samples(N_SILENCE_VAL,   \"sil-val\")\n",
    "    sil_test  = gen_silence_samples(N_SILENCE_TEST,  \"sil-test\")\n",
    "\n",
    "    for feat in sil_train:\n",
    "        X_train.append(feat)\n",
    "        y_train.append(sil_idx)\n",
    "    for feat in sil_val:\n",
    "        X_val.append(feat)\n",
    "        y_val.append(sil_idx)\n",
    "    for feat in sil_test:\n",
    "        X_test.append(feat)\n",
    "        y_test.append(sil_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a61a278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] dataset shapes:\n",
      "  X_train: (36769, 1, 10, 49) y_train: (36769,)\n",
      "  X_val  : (4503, 1, 10, 49) y_val  : (4503,)\n",
      "  X_test : (4874, 1, 10, 49) y_test : (4874,)\n",
      "[DONE] preprocessde data saved to: lab_new/data/kws_12cls_mfcc_10x49.npz\n"
     ]
    }
   ],
   "source": [
    "out_npz = OUT_NPZ\n",
    "X_train = np.stack(X_train, axis=0).astype(np.float32)\n",
    "X_val   = np.stack(X_val,   axis=0).astype(np.float32)\n",
    "X_test  = np.stack(X_test,  axis=0).astype(np.float32)\n",
    "\n",
    "y_train = np.array(y_train, dtype=np.int64)\n",
    "y_val   = np.array(y_val,   dtype=np.int64)\n",
    "y_test  = np.array(y_test,  dtype=np.int64)\n",
    "\n",
    "print(\"\\n[INFO] dataset shapes:\")\n",
    "print(\"  X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"  X_val  :\", X_val.shape,   \"y_val  :\", y_val.shape)\n",
    "print(\"  X_test :\", X_test.shape,  \"y_test :\", y_test.shape)\n",
    "\n",
    "np.savez_compressed(\n",
    "    out_npz,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_valid=X_val,\n",
    "    y_valid=y_val,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    label_names=np.array(LABEL_NAMES)\n",
    ")\n",
    "print(f\"[DONE] preprocessde data saved to: {out_npz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19943d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_npz(npz_path: Path):\n",
    "    print(f\"\\n[INFO] checking :{npz_path}\")\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    X_train = data[\"X_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    X_val   = data[\"X_valid\"]\n",
    "    y_val   = data[\"y_valid\"]\n",
    "    X_test  = data[\"X_test\"]\n",
    "    y_test  = data[\"y_test\"]\n",
    "    label_names = data[\"label_names\"]\n",
    "\n",
    "    print(\"\\n[INFO] label_names:\", label_names)\n",
    "    print(\"\\n[INFO] shape checking:\")\n",
    "    print(\"  X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "    print(\"  X_val  :\", X_val.shape,   \"y_val  :\", y_val.shape)\n",
    "    print(\"  X_test :\", X_test.shape,  \"y_test :\", y_test.shape)\n",
    "\n",
    "    # compute label distribution\n",
    "    def print_label_stats(name, y):\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"\\n  {name} label distribution:\")\n",
    "        for u, c in zip(unique, counts):\n",
    "            lbl = label_names[u] if u < len(label_names) else \"NA\"\n",
    "            print(f\"    idx={u:2d} ({lbl:8s}): {c:6d}\")\n",
    "\n",
    "    print_label_stats(\"Train\", y_train)\n",
    "    print_label_stats(\"Val\",   y_val)\n",
    "    print_label_stats(\"Test\",  y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb360dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] checking :lab_new/data/kws_12cls_mfcc_10x49.npz\n",
      "\n",
      "[INFO] label_names: ['yes' 'no' 'up' 'down' 'left' 'right' 'on' 'off' 'stop' 'go' 'silence'\n",
      " 'unknown']\n",
      "\n",
      "[INFO] shape checking:\n",
      "  X_train: (36769, 1, 10, 49) y_train: (36769,)\n",
      "  X_val  : (4503, 1, 10, 49) y_val  : (4503,)\n",
      "  X_test : (4874, 1, 10, 49) y_test : (4874,)\n",
      "\n",
      "  Train label distribution:\n",
      "    idx= 0 (yes     ):   3228\n",
      "    idx= 1 (no      ):   3130\n",
      "    idx= 2 (up      ):   2948\n",
      "    idx= 3 (down    ):   3134\n",
      "    idx= 4 (left    ):   3037\n",
      "    idx= 5 (right   ):   3019\n",
      "    idx= 6 (on      ):   3086\n",
      "    idx= 7 (off     ):   2970\n",
      "    idx= 8 (stop    ):   3111\n",
      "    idx= 9 (go      ):   3106\n",
      "    idx=10 (silence ):   3000\n",
      "    idx=11 (unknown ):   3000\n",
      "\n",
      "  Val label distribution:\n",
      "    idx= 0 (yes     ):    397\n",
      "    idx= 1 (no      ):    406\n",
      "    idx= 2 (up      ):    350\n",
      "    idx= 3 (down    ):    377\n",
      "    idx= 4 (left    ):    352\n",
      "    idx= 5 (right   ):    363\n",
      "    idx= 6 (on      ):    363\n",
      "    idx= 7 (off     ):    373\n",
      "    idx= 8 (stop    ):    350\n",
      "    idx= 9 (go      ):    372\n",
      "    idx=10 (silence ):    400\n",
      "    idx=11 (unknown ):    400\n",
      "\n",
      "  Test label distribution:\n",
      "    idx= 0 (yes     ):    419\n",
      "    idx= 1 (no      ):    405\n",
      "    idx= 2 (up      ):    425\n",
      "    idx= 3 (down    ):    406\n",
      "    idx= 4 (left    ):    412\n",
      "    idx= 5 (right   ):    396\n",
      "    idx= 6 (on      ):    396\n",
      "    idx= 7 (off     ):    402\n",
      "    idx= 8 (stop    ):    411\n",
      "    idx= 9 (go      ):    402\n",
      "    idx=10 (silence ):    400\n",
      "    idx=11 (unknown ):    400\n"
     ]
    }
   ],
   "source": [
    "inspect_npz(OUT_NPZ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
