{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed74d88",
   "metadata": {},
   "source": [
    "# Lab5: Deep Learning on PYNQ\n",
    "## Scope\n",
    "In the last lab, we have learned how to map a traditional image processing algorithm on the FPGA in a HLS manner.\n",
    "\n",
    "\n",
    "For this lab, we will explore how to deploy a Quantised Neural Network(QNN) on our FPGAs to finish a keyword spotting (KWS).\n",
    "\n",
    "\n",
    "We will finish this task with:\n",
    "- Dataset: Google Speech V2 (preprocessed version, 12 classes, MFCC feature extracted)\n",
    "- Model:   QMLP (3bits)\n",
    "- Board:   PYNQ-Z2 \n",
    "\n",
    "\n",
    "This Lab5 contains 3 parts:\n",
    "- Lab5 A: Train a quantised model and find out the difference between the float NN and the QNN.\n",
    "- Lab5 B: Export the quantised model into a hardware design which could be excuted on our PYNQ board.\n",
    "- Lab5 C: Excute the model in the jupyter notebook to benchmark its performance.\n",
    "\n",
    "## Note\n",
    "We do encourage you to finish this lab in a FINN docker enviroment, but considering limited time, you could also try this in a normal conda/python/colab enviroment.\n",
    "\n",
    "\n",
    "In Lab5B, to generate your own DNN IP, it must be done in the FINN docker. Alternatively, you can also use the generated files provided in the blackboard to continue Lab5 C, or ask TA for a online jupyter sever link with configured enviroment to execute your IP/overlay generation scripts.\n",
    "\n",
    "\n",
    "For what is FINN and how to set up a FINN enviroment, here are some links might be helpful for you:\n",
    "- Enviroment setup: https://github.com/CNStanLee/start_with_finn.git\n",
    "- FINN official docs: https://finn.readthedocs.io/en/latest/\n",
    "- FINN github repo: https://github.com/Xilinx/finn\n",
    "- FINN examples repo: https://github.com/Xilinx/finn-examples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cc54d",
   "metadata": {},
   "source": [
    "# Lab5 A: Train A Quantised Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2698386",
   "metadata": {},
   "source": [
    "## Setup basic enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78173f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  6 03:53:14 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.64.03              Driver Version: 575.64.03      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...    Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   51C    P8              3W /   55W |     468MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2526      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    0   N/A  N/A          254641      C   /usr/bin/python3                        212MiB |\n",
      "|    0   N/A  N/A          311695      C   ...naconda3/envs/lab5/bin/python        220MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9644ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from brevitas.nn import QuantConv2d, QuantLinear, QuantReLU \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41f8a9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/changhong/prj/finn_cli_fork/notebooks/lab_new\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da65e577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "root_path = Path(\".\")  # replace with your root path\n",
    "npz_path = root_path / \"data\" / \"kws_12cls_mfcc_10x49.npz\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c32ad2",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1cc58588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (36769, 1, 10, 49) y_train: (36769,)\n",
      "X_val  : (4503, 1, 10, 49) y_val  : (4503,)\n",
      "X_test : (4874, 1, 10, 49) y_test : (4874,)\n",
      "labels: ['yes' 'no' 'up' 'down' 'left' 'right' 'on' 'off' 'stop' 'go' 'silence'\n",
      " 'unknown']\n",
      "\n",
      "Train label stats:\n",
      "  idx= 0 (yes     ):   3228\n",
      "  idx= 1 (no      ):   3130\n",
      "  idx= 2 (up      ):   2948\n",
      "  idx= 3 (down    ):   3134\n",
      "  idx= 4 (left    ):   3037\n",
      "  idx= 5 (right   ):   3019\n",
      "  idx= 6 (on      ):   3086\n",
      "  idx= 7 (off     ):   2970\n",
      "  idx= 8 (stop    ):   3111\n",
      "  idx= 9 (go      ):   3106\n",
      "  idx=10 (silence ):   3000\n",
      "  idx=11 (unknown ):   3000\n",
      "\n",
      "Val label stats:\n",
      "  idx= 0 (yes     ):    397\n",
      "  idx= 1 (no      ):    406\n",
      "  idx= 2 (up      ):    350\n",
      "  idx= 3 (down    ):    377\n",
      "  idx= 4 (left    ):    352\n",
      "  idx= 5 (right   ):    363\n",
      "  idx= 6 (on      ):    363\n",
      "  idx= 7 (off     ):    373\n",
      "  idx= 8 (stop    ):    350\n",
      "  idx= 9 (go      ):    372\n",
      "  idx=10 (silence ):    400\n",
      "  idx=11 (unknown ):    400\n",
      "\n",
      "Test label stats:\n",
      "  idx= 0 (yes     ):    419\n",
      "  idx= 1 (no      ):    405\n",
      "  idx= 2 (up      ):    425\n",
      "  idx= 3 (down    ):    406\n",
      "  idx= 4 (left    ):    412\n",
      "  idx= 5 (right   ):    396\n",
      "  idx= 6 (on      ):    396\n",
      "  idx= 7 (off     ):    402\n",
      "  idx= 8 (stop    ):    411\n",
      "  idx= 9 (go      ):    402\n",
      "  idx=10 (silence ):    400\n",
      "  idx=11 (unknown ):    400\n"
     ]
    }
   ],
   "source": [
    "data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "X_train = data[\"X_train\"]    # (N_train, 1, 10, 49)\n",
    "y_train = data[\"y_train\"]    # (N_train,)\n",
    "X_val   = data[\"X_valid\"]    # (N_val, 1, 10, 49)\n",
    "y_val   = data[\"y_valid\"]\n",
    "X_test  = data[\"X_test\"]     # (N_test, 1, 10, 49)\n",
    "y_test  = data[\"y_test\"]\n",
    "label_names = data[\"label_names\"]  # ['yes','no',...,'silence','unknown']\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_val  :\", X_val.shape,   \"y_val  :\", y_val.shape)\n",
    "print(\"X_test :\", X_test.shape,  \"y_test :\", y_test.shape)\n",
    "print(\"labels:\", label_names)\n",
    "\n",
    "def print_label_stats(name, y):\n",
    "    uniq, cnt = np.unique(y, return_counts=True)\n",
    "    print(f\"\\n{name} label stats:\")\n",
    "    for u, c in zip(uniq, cnt):\n",
    "        print(f\"  idx={u:2d} ({label_names[u]:8s}): {c:6d}\")\n",
    "\n",
    "print_label_stats(\"Train\", y_train)\n",
    "print_label_stats(\"Val\",   y_val)\n",
    "print_label_stats(\"Test\",  y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "348c515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: -9.154247283935547 std: 73.5042724609375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36769, 4503, 4874)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = X_train.mean()\n",
    "std = X_train.std() + 1e-8\n",
    "\n",
    "X_train_norm = (X_train - mean) / std\n",
    "X_val_norm   = (X_val   - mean) / std\n",
    "X_test_norm  = (X_test  - mean) / std\n",
    "\n",
    "print(\"mean:\", float(mean), \"std:\", float(std))\n",
    "\n",
    "class KWSDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()   # (N, 1, 10, 49)\n",
    "        self.y = torch.from_numpy(y).long()    # (N,)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = KWSDataset(X_train_norm, y_train)\n",
    "val_ds   = KWSDataset(X_val_norm,   y_val)\n",
    "test_ds  = KWSDataset(X_test_norm,  y_test)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e54e85c",
   "metadata": {},
   "source": [
    "## Define the Float Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81d5ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatMLP(nn.Module):\n",
    "    def __init__(self, num_classes=12, hidden_dim=256, dropout_p=0.3):\n",
    "        super().__init__()\n",
    "        self.in_features = 1 * 10 * 49\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.in_features, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8fe54",
   "metadata": {},
   "source": [
    "## Define the Quantised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5101852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantMLPKWS_Dropout(nn.Module):\n",
    "    def __init__(self, num_classes=12, hidden_dim=256, dropout_p=0.2, w_bit=3, a_bit=3):\n",
    "        super().__init__()\n",
    "        self.in_features = 1 * 10 * 49\n",
    "\n",
    "        # Layer 1: 490 -> 256\n",
    "        self.fc1 = QuantLinear(\n",
    "            in_features=self.in_features,\n",
    "            out_features=hidden_dim,\n",
    "            weight_bit_width=w_bit,   # W3\n",
    "            bias=True,\n",
    "            return_quant_tensor=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.act1 = QuantReLU(\n",
    "            bit_width=a_bit,          # A3\n",
    "            return_quant_tensor=False\n",
    "        )\n",
    "        self.drop1 = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        # Layer 2: 256 -> 256\n",
    "        self.fc2 = QuantLinear(\n",
    "            in_features=hidden_dim,\n",
    "            out_features=hidden_dim,\n",
    "            weight_bit_width=w_bit,\n",
    "            bias=True,\n",
    "            return_quant_tensor=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.act2 = QuantReLU(\n",
    "            bit_width=a_bit,\n",
    "            return_quant_tensor=False\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        # Layer 3: 256 -> 256\n",
    "        self.fc3 = QuantLinear(\n",
    "            in_features=hidden_dim,\n",
    "            out_features=hidden_dim,\n",
    "            weight_bit_width=w_bit,\n",
    "            bias=True,\n",
    "            return_quant_tensor=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.act3 = QuantReLU(\n",
    "            bit_width=a_bit,\n",
    "            return_quant_tensor=False\n",
    "        )\n",
    "        self.drop3 = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        # Output layer: 256 -> num_classes\n",
    "        self.fc_out = QuantLinear(\n",
    "            in_features=hidden_dim,\n",
    "            out_features=num_classes,\n",
    "            weight_bit_width=w_bit,\n",
    "            bias=True,\n",
    "            return_quant_tensor=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 10, 49) -> flatten -> (B, 490)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9fbd86",
   "metadata": {},
   "source": [
    "## Train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42cd1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader = None,\n",
    "        test_loader: DataLoader = None,\n",
    "        device: torch.device = None,\n",
    "        # --- Hyperparameters ---\n",
    "        lr: float = 3e-4,\n",
    "        weight_decay: float = 1e-4,\n",
    "        batch_size: int = 64,\n",
    "        num_epochs: int = 100,\n",
    "        scheduler_factor: float = 0.5,\n",
    "        scheduler_patience: int = 3,\n",
    "        optimizer_cls=torch.optim.Adam,\n",
    "        criterion: nn.Module = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A simple training framework for classification tasks.\n",
    "\n",
    "        Args:\n",
    "            model: Neural network model (nn.Module)\n",
    "            train_loader: DataLoader for training set\n",
    "            val_loader: DataLoader for validation set\n",
    "            test_loader: DataLoader for test set (optional)\n",
    "            device: torch.device (if None, automatically selects cuda or cpu)\n",
    "            lr: Learning rate\n",
    "            weight_decay: Weight decay (L2 regularization)\n",
    "            batch_size: Batch size (for reference or logging)\n",
    "            num_epochs: Number of training epochs\n",
    "            scheduler_factor: Factor by which LR is reduced (ReduceLROnPlateau)\n",
    "            scheduler_patience: Number of epochs with no improvement before LR reduction\n",
    "            optimizer_cls: Optimizer class (e.g., Adam, SGD)\n",
    "            criterion: Loss function (default: CrossEntropyLoss)\n",
    "        \"\"\"\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        # --- Save hyperparameters ---\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.scheduler_factor = scheduler_factor\n",
    "        self.scheduler_patience = scheduler_patience\n",
    "\n",
    "        # --- Training components ---\n",
    "        self.criterion = criterion or nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer_cls(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # Scheduler triggered by validation accuracy\n",
    "        if self.val_loader is not None:\n",
    "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer,\n",
    "                mode=\"max\",\n",
    "                factor=scheduler_factor,\n",
    "                patience=scheduler_patience,\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "        # --- Bookkeeping ---\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_state_dict = None\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "        }\n",
    "\n",
    "    def _run_one_epoch(self, loader, train: bool = True):\n",
    "        \"\"\"\n",
    "        Run one epoch of training or evaluation.\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        context = torch.enable_grad() if train else torch.no_grad()\n",
    "        with context:\n",
    "            for X, y in loader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "                if train:\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                logits = self.model(X)\n",
    "                loss = self.criterion(logits, y)\n",
    "\n",
    "                if train:\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * X.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                total_correct += (preds == y).sum().item()\n",
    "                total_samples += X.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total_samples\n",
    "        acc = total_correct / total_samples\n",
    "        return avg_loss, acc\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "        Tracks and reports both training and validation performance.\n",
    "        \"\"\"\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            train_loss, train_acc = self._run_one_epoch(self.train_loader, train=True)\n",
    "\n",
    "            if self.val_loader is not None:\n",
    "                val_loss, val_acc = self._run_one_epoch(self.val_loader, train=False)\n",
    "\n",
    "                # Step the LR scheduler based on validation accuracy\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step(val_acc)\n",
    "\n",
    "                # Track best model\n",
    "                if val_acc > self.best_val_acc:\n",
    "                    self.best_val_acc = val_acc\n",
    "                    self.best_state_dict = {\n",
    "                        k: v.cpu().clone() for k, v in self.model.state_dict().items()\n",
    "                    }\n",
    "\n",
    "                # Log metrics\n",
    "                self.history[\"train_loss\"].append(train_loss)\n",
    "                self.history[\"train_acc\"].append(train_acc)\n",
    "                self.history[\"val_loss\"].append(val_loss)\n",
    "                self.history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch {epoch:02d}/{self.num_epochs} | \"\n",
    "                    f\"Train Loss={train_loss:.4f}, Train Acc={train_acc*100:5.2f}% | \"\n",
    "                    f\"Val Loss={val_loss:.4f}, Val Acc={val_acc*100:5.2f}%\"\n",
    "                )\n",
    "            else:\n",
    "                # No validation set\n",
    "                self.history[\"train_loss\"].append(train_loss)\n",
    "                self.history[\"train_acc\"].append(train_acc)\n",
    "                print(\n",
    "                    f\"Epoch {epoch:02d}/{self.num_epochs} | \"\n",
    "                    f\"Train Loss={train_loss:.4f}, Train Acc={train_acc*100:5.2f}%\"\n",
    "                )\n",
    "\n",
    "        if self.val_loader is not None:\n",
    "            print(f\"\\n[INFO] Best Validation Accuracy = {self.best_val_acc*100:.2f}%\")\n",
    "\n",
    "    def load_best_model(self):\n",
    "        \"\"\"\n",
    "        Restore the best-performing model parameters (based on validation accuracy).\n",
    "        \"\"\"\n",
    "        if self.best_state_dict is not None:\n",
    "            self.model.load_state_dict(self.best_state_dict)\n",
    "            self.model.to(self.device)\n",
    "        else:\n",
    "            print(\"[WARN] No best_state_dict found. Ensure validation was used during training.\")\n",
    "\n",
    "    def test(self, test_loader: DataLoader = None):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test set.\n",
    "        Automatically loads the best checkpoint if available.\n",
    "        \"\"\"\n",
    "        loader = test_loader or self.test_loader\n",
    "        if loader is None:\n",
    "            raise ValueError(\"No test_loader provided.\")\n",
    "\n",
    "        # Use the best model checkpoint if available\n",
    "        if self.best_state_dict is not None:\n",
    "            self.load_best_model()\n",
    "\n",
    "        test_loss, test_acc = self._run_one_epoch(loader, train=False)\n",
    "        print(f\"[TEST] Loss={test_loss:.4f}, Accuracy={test_acc*100:5.2f}%\")\n",
    "        return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1044f",
   "metadata": {},
   "source": [
    "## Train the float model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe538867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train Loss=1.7510, Train Acc=40.80% | Val Loss=1.1954, Val Acc=62.29%\n",
      "Epoch 02/20 | Train Loss=1.1726, Train Acc=60.30% | Val Loss=0.9605, Val Acc=68.07%\n",
      "Epoch 03/20 | Train Loss=1.0007, Train Acc=66.06% | Val Loss=0.8484, Val Acc=71.51%\n",
      "Epoch 04/20 | Train Loss=0.9047, Train Acc=69.21% | Val Loss=0.7864, Val Acc=73.84%\n",
      "Epoch 05/20 | Train Loss=0.8379, Train Acc=71.27% | Val Loss=0.7826, Val Acc=73.82%\n",
      "Epoch 06/20 | Train Loss=0.7862, Train Acc=73.08% | Val Loss=0.7649, Val Acc=73.75%\n",
      "Epoch 07/20 | Train Loss=0.7449, Train Acc=74.61% | Val Loss=0.7069, Val Acc=76.33%\n",
      "Epoch 08/20 | Train Loss=0.7066, Train Acc=75.69% | Val Loss=0.7162, Val Acc=76.99%\n",
      "Epoch 09/20 | Train Loss=0.6768, Train Acc=76.96% | Val Loss=0.7008, Val Acc=76.88%\n",
      "Epoch 10/20 | Train Loss=0.6551, Train Acc=77.67% | Val Loss=0.6855, Val Acc=76.77%\n",
      "Epoch 11/20 | Train Loss=0.6294, Train Acc=78.63% | Val Loss=0.6749, Val Acc=77.13%\n",
      "Epoch 12/20 | Train Loss=0.6057, Train Acc=79.25% | Val Loss=0.6435, Val Acc=78.55%\n",
      "Epoch 13/20 | Train Loss=0.5877, Train Acc=79.72% | Val Loss=0.6474, Val Acc=78.95%\n",
      "Epoch 14/20 | Train Loss=0.5752, Train Acc=80.10% | Val Loss=0.6419, Val Acc=79.21%\n",
      "Epoch 15/20 | Train Loss=0.5518, Train Acc=80.84% | Val Loss=0.6473, Val Acc=79.08%\n",
      "Epoch 16/20 | Train Loss=0.5355, Train Acc=81.65% | Val Loss=0.6422, Val Acc=79.21%\n",
      "Epoch 17/20 | Train Loss=0.5185, Train Acc=82.12% | Val Loss=0.6630, Val Acc=79.30%\n",
      "Epoch 18/20 | Train Loss=0.5096, Train Acc=82.39% | Val Loss=0.6351, Val Acc=79.59%\n",
      "Epoch 19/20 | Train Loss=0.5006, Train Acc=82.79% | Val Loss=0.6420, Val Acc=79.48%\n",
      "Epoch 20/20 | Train Loss=0.4824, Train Acc=83.27% | Val Loss=0.6406, Val Acc=79.70%\n",
      "\n",
      "[INFO] Best Validation Accuracy = 79.70%\n",
      "[TEST] Loss=0.6449, Accuracy=78.44%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6448682725960941, 0.7843660237997538)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FloatMLP(num_classes=12, hidden_dim=256, dropout_p=0.3).to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=256,\n",
    "    num_epochs=20,\n",
    "    scheduler_factor=0.5,\n",
    "    scheduler_patience=3,\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30148270",
   "metadata": {},
   "source": [
    "## Train the quantised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d82cb68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train Loss=1.8032, Train Acc=38.40% | Val Loss=1.3151, Val Acc=57.61%\n",
      "Epoch 02/20 | Train Loss=1.2500, Train Acc=56.89% | Val Loss=1.0937, Val Acc=64.31%\n",
      "Epoch 03/20 | Train Loss=1.0842, Train Acc=62.96% | Val Loss=1.0076, Val Acc=65.56%\n",
      "Epoch 04/20 | Train Loss=0.9974, Train Acc=66.11% | Val Loss=0.9591, Val Acc=67.69%\n",
      "Epoch 05/20 | Train Loss=0.9333, Train Acc=67.83% | Val Loss=0.9265, Val Acc=68.53%\n",
      "Epoch 06/20 | Train Loss=0.8933, Train Acc=69.62% | Val Loss=0.9504, Val Acc=67.67%\n",
      "Epoch 07/20 | Train Loss=0.8607, Train Acc=70.69% | Val Loss=1.0692, Val Acc=64.18%\n",
      "Epoch 08/20 | Train Loss=0.8324, Train Acc=71.63% | Val Loss=0.9145, Val Acc=68.31%\n",
      "Epoch 09/20 | Train Loss=0.8032, Train Acc=72.74% | Val Loss=0.8917, Val Acc=70.06%\n",
      "Epoch 10/20 | Train Loss=0.7846, Train Acc=73.35% | Val Loss=0.9244, Val Acc=68.11%\n",
      "Epoch 11/20 | Train Loss=0.7708, Train Acc=73.36% | Val Loss=0.8123, Val Acc=73.13%\n",
      "Epoch 12/20 | Train Loss=0.7526, Train Acc=74.08% | Val Loss=0.9469, Val Acc=68.73%\n",
      "Epoch 13/20 | Train Loss=0.7423, Train Acc=74.51% | Val Loss=0.8209, Val Acc=73.13%\n",
      "Epoch 14/20 | Train Loss=0.7167, Train Acc=75.56% | Val Loss=0.9027, Val Acc=70.18%\n",
      "Epoch 15/20 | Train Loss=0.7081, Train Acc=75.73% | Val Loss=1.1075, Val Acc=64.22%\n",
      "Epoch 16/20 | Train Loss=0.6705, Train Acc=77.10% | Val Loss=0.8116, Val Acc=72.88%\n",
      "Epoch 17/20 | Train Loss=0.6631, Train Acc=77.32% | Val Loss=0.7544, Val Acc=75.84%\n",
      "Epoch 18/20 | Train Loss=0.6536, Train Acc=77.38% | Val Loss=0.7466, Val Acc=74.71%\n",
      "Epoch 19/20 | Train Loss=0.6436, Train Acc=77.88% | Val Loss=0.7508, Val Acc=74.86%\n",
      "Epoch 20/20 | Train Loss=0.6372, Train Acc=78.25% | Val Loss=0.8968, Val Acc=71.24%\n",
      "\n",
      "[INFO] Best Validation Accuracy = 75.84%\n",
      "[TEST] Loss=0.7303, Accuracy=74.74%\n"
     ]
    }
   ],
   "source": [
    "model = QuantMLPKWS_Dropout(\n",
    "    num_classes=12,\n",
    "    hidden_dim=256,\n",
    "    dropout_p=0.3,   \n",
    "    w_bit=3, a_bit=3\n",
    ").to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=256,\n",
    "    num_epochs=20,\n",
    "    scheduler_factor=0.5,\n",
    "    scheduler_patience=3,\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.test()\n",
    "\n",
    "# save weights\n",
    "weight_dir = root_path / \"weights\"\n",
    "weight_dir.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), weight_dir / \"mlpw2a2_model_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4e752",
   "metadata": {},
   "source": [
    "### Find out what is the difference and how it is relatated to the bitwidth, how to select bitwidth?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63275ca4",
   "metadata": {},
   "source": [
    "## Try other bit width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac83602d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train Loss=1.8819, Train Acc=35.23% | Val Loss=1.3294, Val Acc=56.21%\n",
      "Epoch 02/20 | Train Loss=1.2860, Train Acc=55.62% | Val Loss=1.0788, Val Acc=64.40%\n",
      "Epoch 03/20 | Train Loss=1.0936, Train Acc=62.68% | Val Loss=1.0128, Val Acc=65.78%\n",
      "Epoch 04/20 | Train Loss=0.9997, Train Acc=66.24% | Val Loss=0.9131, Val Acc=69.40%\n",
      "Epoch 05/20 | Train Loss=0.9326, Train Acc=68.36% | Val Loss=0.8825, Val Acc=69.89%\n",
      "Epoch 06/20 | Train Loss=0.8878, Train Acc=69.89% | Val Loss=0.9051, Val Acc=69.49%\n",
      "Epoch 07/20 | Train Loss=0.8409, Train Acc=71.36% | Val Loss=0.8197, Val Acc=72.20%\n",
      "Epoch 08/20 | Train Loss=0.8088, Train Acc=72.13% | Val Loss=0.8062, Val Acc=73.02%\n",
      "Epoch 09/20 | Train Loss=0.7802, Train Acc=73.52% | Val Loss=0.8071, Val Acc=73.44%\n",
      "Epoch 10/20 | Train Loss=0.7543, Train Acc=74.11% | Val Loss=0.7677, Val Acc=74.84%\n",
      "Epoch 11/20 | Train Loss=0.7320, Train Acc=74.99% | Val Loss=0.8047, Val Acc=73.11%\n",
      "Epoch 12/20 | Train Loss=0.7160, Train Acc=75.55% | Val Loss=0.8042, Val Acc=73.80%\n",
      "Epoch 13/20 | Train Loss=0.6952, Train Acc=76.42% | Val Loss=0.7494, Val Acc=75.22%\n",
      "Epoch 14/20 | Train Loss=0.6828, Train Acc=76.79% | Val Loss=0.7708, Val Acc=74.37%\n",
      "Epoch 15/20 | Train Loss=0.6750, Train Acc=77.17% | Val Loss=0.7410, Val Acc=75.26%\n",
      "Epoch 16/20 | Train Loss=0.6544, Train Acc=77.78% | Val Loss=0.7524, Val Acc=75.59%\n",
      "Epoch 17/20 | Train Loss=0.6420, Train Acc=78.19% | Val Loss=0.7647, Val Acc=74.77%\n",
      "Epoch 18/20 | Train Loss=0.6276, Train Acc=78.58% | Val Loss=0.8063, Val Acc=73.97%\n",
      "Epoch 19/20 | Train Loss=0.6129, Train Acc=79.03% | Val Loss=0.7381, Val Acc=75.86%\n",
      "Epoch 20/20 | Train Loss=0.6102, Train Acc=79.33% | Val Loss=0.7508, Val Acc=74.97%\n",
      "\n",
      "[INFO] Best Validation Accuracy = 75.86%\n",
      "[TEST] Loss=0.7483, Accuracy=75.67%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.748250197800509, 0.756668034468609)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_weight_bitwidth = 4  # Example: change to 4 bits\n",
    "your_activation_bitwidth = 2  # Example: change to 4 bits\n",
    "\n",
    "\n",
    "model = QuantMLPKWS_Dropout(\n",
    "    num_classes=12,\n",
    "    hidden_dim=256,\n",
    "    dropout_p=0.3,   \n",
    "    w_bit=your_weight_bitwidth, a_bit=your_activation_bitwidth\n",
    ").to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=256,\n",
    "    num_epochs=20,\n",
    "    scheduler_factor=0.5,\n",
    "    scheduler_patience=3,\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aa7c4c",
   "metadata": {},
   "source": [
    "### Report your accuracy and its corresponding bit width.\n",
    "### Note: you could fine tune the hyper parameters, higher accuracy with smaller model is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6e565",
   "metadata": {},
   "source": [
    "## Export your weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0be07c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
