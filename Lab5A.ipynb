{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6ed74d88",
      "metadata": {
        "id": "6ed74d88"
      },
      "source": [
        "# Lab5: Deep Learning on PYNQ\n",
        "## Scope\n",
        "In the last lab, we have learned how to map a traditional image processing algorithm on the FPGA in a HLS manner.\n",
        "\n",
        "\n",
        "For this lab, we will explore how to deploy a Quantised Neural Network(QNN) on our FPGAs to finish a keyword spotting (KWS).\n",
        "\n",
        "\n",
        "We will finish this task with:\n",
        "- Dataset: Google Speech V2 (preprocessed version, 12 classes, MFCC feature extracted)\n",
        "- Model:   QMLP (3bits)\n",
        "- Board:   PYNQ-Z2\n",
        "\n",
        "\n",
        "This Lab5 contains 3 parts:\n",
        "- Lab5 A: Train a quantised model and find out the difference between the float NN and the QNN.\n",
        "- Lab5 B (optional): Export the quantised model into a hardware design which could be excuted on our PYNQ board.\n",
        "- Lab5 C: Excute the model in the jupyter notebook to benchmark its performance.\n",
        "\n",
        "## Note\n",
        "We do encourage you to finish this lab in a FINN docker enviroment, but considering limited time, you could also try this in a normal conda/python/colab enviroment.\n",
        "\n",
        "\n",
        "In Lab5B, to generate your own DNN IP, it must be done in the FINN docker. Alternatively, you can also use the generated files provided in the blackboard to continue Lab5 C, or ask TA for a online jupyter sever link with configured enviroment to execute your IP/overlay generation scripts.\n",
        "\n",
        "\n",
        "For what is FINN and how to set up a FINN enviroment, here are some links might be helpful for you:\n",
        "- Enviroment setup: https://github.com/CNStanLee/start_with_finn.git\n",
        "- FINN official docs: https://finn.readthedocs.io/en/latest/\n",
        "- FINN github repo: https://github.com/Xilinx/finn\n",
        "- FINN examples repo: https://github.com/Xilinx/finn-examples\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e42cc54d",
      "metadata": {
        "id": "e42cc54d"
      },
      "source": [
        "# Lab5 A: Train A Quantised Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2698386",
      "metadata": {
        "id": "f2698386"
      },
      "source": [
        "## Setup basic enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "78173f5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78173f5d",
        "outputId": "7b43462d-8b8f-4c16-d8d3-5ca3c4a3504f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install brevitas==0.12.1\n",
        "! pip install onnx\n",
        "! pip install onnxscript\n",
        "! pip install qonnx\n",
        "! pip install onnxoptimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bFufJvH103Y",
        "outputId": "2e5b22d5-c3cf-4113-ed6f-75cc81d0410c"
      },
      "id": "8bFufJvH103Y",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: brevitas==0.12.1 in /usr/local/lib/python3.12/dist-packages (0.12.1)\n",
            "Requirement already satisfied: dependencies==2.0.1 in /usr/local/lib/python3.12/dist-packages (from brevitas==0.12.1) (2.0.1)\n",
            "Requirement already satisfied: numpy<=1.26.4 in /usr/local/lib/python3.12/dist-packages (from brevitas==0.12.1) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from brevitas==0.12.1) (25.0)\n",
            "Requirement already satisfied: setuptools<70.0 in /usr/local/lib/python3.12/dist-packages (from brevitas==0.12.1) (69.5.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from brevitas==0.12.1) (1.13.3)\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.12/dist-packages (from brevitas==0.12.1) (2.8.0+cu126)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from brevitas==0.12.1) (4.15.0)\n",
            "Requirement already satisfied: unfoldNd in /usr/local/lib/python3.12/dist-packages (from brevitas==0.12.1) (0.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (3.20.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12->brevitas==0.12.1) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->brevitas==0.12.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.12->brevitas==0.12.1) (3.0.3)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.12/dist-packages (0.5.6)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from onnxscript) (1.26.4)\n",
            "Requirement already satisfied: onnx_ir<2,>=0.1.12 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.1.12)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (25.0)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.15.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.16->onnxscript) (3.20.3)\n",
            "Requirement already satisfied: qonnx in /usr/local/lib/python3.12/dist-packages (0.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from qonnx) (25.4.0)\n",
            "Requirement already satisfied: clize>=5.0.1 in /usr/local/lib/python3.12/dist-packages (from qonnx) (5.0.2)\n",
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.12/dist-packages (from qonnx) (3.20.3)\n",
            "Requirement already satisfied: bitstring>=3.1.7 in /usr/local/lib/python3.12/dist-packages (from qonnx) (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.12/dist-packages (from qonnx) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from qonnx) (1.17.0)\n",
            "Requirement already satisfied: onnxruntime>=1.16.1 in /usr/local/lib/python3.12/dist-packages (from qonnx) (1.23.2)\n",
            "Requirement already satisfied: sigtools>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from qonnx) (4.0.1)\n",
            "Requirement already satisfied: toposort>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from qonnx) (1.10)\n",
            "Requirement already satisfied: bitarray<4.0,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bitstring>=3.1.7->qonnx) (3.8.0)\n",
            "Requirement already satisfied: od in /usr/local/lib/python3.12/dist-packages (from clize>=5.0.1->qonnx) (2.0.2)\n",
            "Requirement already satisfied: docutils>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from clize>=5.0.1->qonnx) (0.21.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.16.1->qonnx) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.16.1->qonnx) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.16.1->qonnx) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.16.1->qonnx) (1.13.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.16.1->qonnx) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.16.1->qonnx) (1.3.0)\n",
            "Collecting onnxoptimizer\n",
            "  Using cached onnxoptimizer-0.3.13.tar.gz (18.5 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (from onnxoptimizer) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from onnx->onnxoptimizer) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from onnx->onnxoptimizer) (3.20.3)\n",
            "Building wheels for collected packages: onnxoptimizer\n",
            "  Building wheel for onnxoptimizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for onnxoptimizer: filename=onnxoptimizer-0.3.13-cp312-cp312-linux_x86_64.whl size=621626 sha256=e1520faf3414255f1312f385c11513aa02b0f90476934c17a4872c0b0a4516ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/75/fa/11d91817e007f50a1d716a0e5fe977c3990c4701786d7c5a04\n",
            "Successfully built onnxoptimizer\n",
            "Installing collected packages: onnxoptimizer\n",
            "Successfully installed onnxoptimizer-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b9644ab2",
      "metadata": {
        "id": "b9644ab2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from brevitas.nn import QuantConv2d, QuantLinear, QuantReLU\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "41f8a9cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41f8a9cf",
        "outputId": "568aada4-4b47-4032-cf71-865856dbc01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "! pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "da65e577",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da65e577",
        "outputId": "264464f3-793b-45e0-bb49-c75af785ec30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "root_path = Path(\"lab_new\")  # replace with your root path\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "npz_path = root_path / \"data\" / \"kws_12cls_mfcc_10x49.npz\"\n",
        "data_download_link = 'https://drive.google.com/file/d/1ndk0v3vCNPMWtzx9Kubg3jqYRRqDc-55/view?usp=sharing'\n",
        "# download from google drive to npz_path if the file not existed"
      ],
      "metadata": {
        "id": "gSIYmR-L2MtP"
      },
      "id": "gSIYmR-L2MtP",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import sys, subprocess\n",
        "\n",
        "npz_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _ensure_gdown():\n",
        "    try:\n",
        "        import gdown\n",
        "        return gdown\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gdown\"])\n",
        "        import gdown\n",
        "        return gdown\n",
        "\n",
        "if not npz_path.exists():\n",
        "    gdown = _ensure_gdown()\n",
        "    out = gdown.download(url=data_download_link, output=str(npz_path), quiet=False, fuzzy=True)\n",
        "    if not out or not Path(out).exists():\n",
        "        raise RuntimeError(f\"down load failed: {npz_path}\")\n",
        "    print(f\"Downloaded to: {npz_path}\")\n",
        "else:\n",
        "    print(f\"File already exists: {npz_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JWUYixz3sYf",
        "outputId": "262edc0c-73a3-48bb-a6c5-30b916a01a71"
      },
      "id": "5JWUYixz3sYf",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists: lab_new/data/kws_12cls_mfcc_10x49.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c32ad2",
      "metadata": {
        "id": "b8c32ad2"
      },
      "source": [
        "## Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1cc58588",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cc58588",
        "outputId": "39b5dd57-6233-4645-82c4-5175eafea80a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (36769, 1, 10, 49) y_train: (36769,)\n",
            "X_val  : (4503, 1, 10, 49) y_val  : (4503,)\n",
            "X_test : (4874, 1, 10, 49) y_test : (4874,)\n",
            "labels: ['yes' 'no' 'up' 'down' 'left' 'right' 'on' 'off' 'stop' 'go' 'silence'\n",
            " 'unknown']\n",
            "\n",
            "Train label stats:\n",
            "  idx= 0 (yes     ):   3228\n",
            "  idx= 1 (no      ):   3130\n",
            "  idx= 2 (up      ):   2948\n",
            "  idx= 3 (down    ):   3134\n",
            "  idx= 4 (left    ):   3037\n",
            "  idx= 5 (right   ):   3019\n",
            "  idx= 6 (on      ):   3086\n",
            "  idx= 7 (off     ):   2970\n",
            "  idx= 8 (stop    ):   3111\n",
            "  idx= 9 (go      ):   3106\n",
            "  idx=10 (silence ):   3000\n",
            "  idx=11 (unknown ):   3000\n",
            "\n",
            "Val label stats:\n",
            "  idx= 0 (yes     ):    397\n",
            "  idx= 1 (no      ):    406\n",
            "  idx= 2 (up      ):    350\n",
            "  idx= 3 (down    ):    377\n",
            "  idx= 4 (left    ):    352\n",
            "  idx= 5 (right   ):    363\n",
            "  idx= 6 (on      ):    363\n",
            "  idx= 7 (off     ):    373\n",
            "  idx= 8 (stop    ):    350\n",
            "  idx= 9 (go      ):    372\n",
            "  idx=10 (silence ):    400\n",
            "  idx=11 (unknown ):    400\n",
            "\n",
            "Test label stats:\n",
            "  idx= 0 (yes     ):    419\n",
            "  idx= 1 (no      ):    405\n",
            "  idx= 2 (up      ):    425\n",
            "  idx= 3 (down    ):    406\n",
            "  idx= 4 (left    ):    412\n",
            "  idx= 5 (right   ):    396\n",
            "  idx= 6 (on      ):    396\n",
            "  idx= 7 (off     ):    402\n",
            "  idx= 8 (stop    ):    411\n",
            "  idx= 9 (go      ):    402\n",
            "  idx=10 (silence ):    400\n",
            "  idx=11 (unknown ):    400\n"
          ]
        }
      ],
      "source": [
        "data = np.load(npz_path, allow_pickle=True)\n",
        "\n",
        "X_train = data[\"X_train\"]    # (N_train, 1, 10, 49)\n",
        "y_train = data[\"y_train\"]    # (N_train,)\n",
        "X_val   = data[\"X_valid\"]    # (N_val, 1, 10, 49)\n",
        "y_val   = data[\"y_valid\"]\n",
        "X_test  = data[\"X_test\"]     # (N_test, 1, 10, 49)\n",
        "y_test  = data[\"y_test\"]\n",
        "label_names = data[\"label_names\"]  # ['yes','no',...,'silence','unknown']\n",
        "\n",
        "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
        "print(\"X_val  :\", X_val.shape,   \"y_val  :\", y_val.shape)\n",
        "print(\"X_test :\", X_test.shape,  \"y_test :\", y_test.shape)\n",
        "print(\"labels:\", label_names)\n",
        "\n",
        "def print_label_stats(name, y):\n",
        "    uniq, cnt = np.unique(y, return_counts=True)\n",
        "    print(f\"\\n{name} label stats:\")\n",
        "    for u, c in zip(uniq, cnt):\n",
        "        print(f\"  idx={u:2d} ({label_names[u]:8s}): {c:6d}\")\n",
        "\n",
        "print_label_stats(\"Train\", y_train)\n",
        "print_label_stats(\"Val\",   y_val)\n",
        "print_label_stats(\"Test\",  y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "348c515a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "348c515a",
        "outputId": "807207c5-c7b4-40fe-b293-4fc05f2288ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean: -9.13790512084961 std: 73.45867157982421\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36769, 4503, 4874)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "mean = X_train.mean()\n",
        "std = X_train.std() + 1e-8\n",
        "\n",
        "X_train_norm = (X_train - mean) / std\n",
        "X_val_norm   = (X_val   - mean) / std\n",
        "X_test_norm  = (X_test  - mean) / std\n",
        "\n",
        "print(\"mean:\", float(mean), \"std:\", float(std))\n",
        "\n",
        "class KWSDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()   # (N, 1, 10, 49)\n",
        "        self.y = torch.from_numpy(y).long()    # (N,)\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_ds = KWSDataset(X_train_norm, y_train)\n",
        "val_ds   = KWSDataset(X_val_norm,   y_val)\n",
        "test_ds  = KWSDataset(X_test_norm,  y_test)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=False)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "len(train_ds), len(val_ds), len(test_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e54e85c",
      "metadata": {
        "id": "6e54e85c"
      },
      "source": [
        "## Define the Float Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "81d5ef39",
      "metadata": {
        "id": "81d5ef39"
      },
      "outputs": [],
      "source": [
        "class FloatMLP(nn.Module):\n",
        "    def __init__(self, num_classes=12, hidden_dim=256, dropout_p=0.3):\n",
        "        super().__init__()\n",
        "        self.in_features = 1 * 10 * 49\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.in_features, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81c8fe54",
      "metadata": {
        "id": "81c8fe54"
      },
      "source": [
        "## Define the Quantised Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5101852a",
      "metadata": {
        "id": "5101852a"
      },
      "outputs": [],
      "source": [
        "from brevitas.nn import QuantIdentity\n",
        "\n",
        "class QuantMLPKWS_Dropout(nn.Module):\n",
        "    def __init__(self, num_classes=12, hidden_dim=256, dropout_p=0.2,\n",
        "                 w_bit=3, a_bit=3, in_bit=8):\n",
        "        super().__init__()\n",
        "        self.in_features = 1 * 10 * 49\n",
        "\n",
        "        # self.input_quant = QuantIdentity(\n",
        "        #     bit_width=in_bit,        # 8\n",
        "        #     return_quant_tensor=False\n",
        "        # )\n",
        "\n",
        "        # Layer 1: 490 -> 256\n",
        "        self.fc1 = QuantLinear(\n",
        "            in_features=self.in_features,\n",
        "            out_features=hidden_dim,\n",
        "            weight_bit_width=w_bit,   # W3\n",
        "            bias=True,\n",
        "            return_quant_tensor=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.act1 = QuantReLU(\n",
        "            bit_width=a_bit,          # A3\n",
        "            return_quant_tensor=False\n",
        "        )\n",
        "        self.drop1 = nn.Dropout(p=dropout_p)\n",
        "\n",
        "        # Layer 2: 256 -> 256\n",
        "        self.fc2 = QuantLinear(\n",
        "            in_features=hidden_dim,\n",
        "            out_features=hidden_dim,\n",
        "            weight_bit_width=w_bit,\n",
        "            bias=True,\n",
        "            return_quant_tensor=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.act2 = QuantReLU(\n",
        "            bit_width=a_bit,\n",
        "            return_quant_tensor=False\n",
        "        )\n",
        "        self.drop2 = nn.Dropout(p=dropout_p)\n",
        "\n",
        "        # Layer 3: 256 -> 256\n",
        "        self.fc3 = QuantLinear(\n",
        "            in_features=hidden_dim,\n",
        "            out_features=hidden_dim,\n",
        "            weight_bit_width=w_bit,\n",
        "            bias=True,\n",
        "            return_quant_tensor=False\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.act3 = QuantReLU(\n",
        "            bit_width=a_bit,\n",
        "            return_quant_tensor=False\n",
        "        )\n",
        "        self.drop3 = nn.Dropout(p=dropout_p)\n",
        "\n",
        "        # Output layer: 256 -> num_classes\n",
        "        self.fc_out = QuantLinear(\n",
        "            in_features=hidden_dim,\n",
        "            out_features=num_classes,\n",
        "            weight_bit_width=w_bit,\n",
        "            bias=True,\n",
        "            return_quant_tensor=False\n",
        "        )\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, 1, 10, 49)\n",
        "        # x = self.input_quant(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.drop2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.act3(x)\n",
        "        x = self.drop3(x)\n",
        "\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df9fbd86",
      "metadata": {
        "id": "df9fbd86"
      },
      "source": [
        "## Train functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "42cd1c3a",
      "metadata": {
        "id": "42cd1c3a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader = None,\n",
        "        test_loader: DataLoader = None,\n",
        "        device: torch.device = None,\n",
        "        # --- Hyperparameters ---\n",
        "        lr: float = 3e-4,\n",
        "        weight_decay: float = 1e-4,\n",
        "        batch_size: int = 64,\n",
        "        num_epochs: int = 100,\n",
        "        scheduler_factor: float = 0.5,\n",
        "        scheduler_patience: int = 3,\n",
        "        optimizer_cls=torch.optim.Adam,\n",
        "        criterion: nn.Module = None,\n",
        "        use_input_quant = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        A simple training framework for classification tasks.\n",
        "\n",
        "        Args:\n",
        "            model: Neural network model (nn.Module)\n",
        "            train_loader: DataLoader for training set\n",
        "            val_loader: DataLoader for validation set\n",
        "            test_loader: DataLoader for test set (optional)\n",
        "            device: torch.device (if None, automatically selects cuda or cpu)\n",
        "            lr: Learning rate\n",
        "            weight_decay: Weight decay (L2 regularization)\n",
        "            batch_size: Batch size (for reference or logging)\n",
        "            num_epochs: Number of training epochs\n",
        "            scheduler_factor: Factor by which LR is reduced (ReduceLROnPlateau)\n",
        "            scheduler_patience: Number of epochs with no improvement before LR reduction\n",
        "            optimizer_cls: Optimizer class (e.g., Adam, SGD)\n",
        "            criterion: Loss function (default: CrossEntropyLoss)\n",
        "        \"\"\"\n",
        "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.use_input_quant = use_input_quant\n",
        "\n",
        "        # --- Save hyperparameters ---\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.scheduler_factor = scheduler_factor\n",
        "        self.scheduler_patience = scheduler_patience\n",
        "\n",
        "        # --- Training components ---\n",
        "        self.criterion = criterion or nn.CrossEntropyLoss()\n",
        "        self.optimizer = optimizer_cls(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        # Scheduler triggered by validation accuracy\n",
        "        if self.val_loader is not None:\n",
        "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                self.optimizer,\n",
        "                mode=\"max\",\n",
        "                factor=scheduler_factor,\n",
        "                patience=scheduler_patience,\n",
        "            )\n",
        "        else:\n",
        "            self.scheduler = None\n",
        "\n",
        "        # --- Bookkeeping ---\n",
        "        self.best_val_acc = 0.0\n",
        "        self.best_state_dict = None\n",
        "        self.history = {\n",
        "            \"train_loss\": [],\n",
        "            \"train_acc\": [],\n",
        "            \"val_loss\": [],\n",
        "            \"val_acc\": [],\n",
        "        }\n",
        "\n",
        "    def _quantize_input(self, X):\n",
        "        max_val = X.abs().max()\n",
        "\n",
        "        if max_val == 0:\n",
        "            scale = 1.0\n",
        "        else:\n",
        "            scale = max_val / 127.0\n",
        "        X_int = torch.round(X / scale).clamp(-128, 127)\n",
        "\n",
        "        return X_int\n",
        "\n",
        "\n",
        "    def _run_one_epoch(self, loader, train: bool = True):\n",
        "        \"\"\"\n",
        "        Run one epoch of training or evaluation.\n",
        "        \"\"\"\n",
        "        if train:\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        context = torch.enable_grad() if train else torch.no_grad()\n",
        "        with context:\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(self.device), y.to(self.device)\n",
        "                if self.use_input_quant:\n",
        "                    X = self._quantize_input(X)\n",
        "                if train:\n",
        "                    self.optimizer.zero_grad()\n",
        "\n",
        "                logits = self.model(X)\n",
        "                loss = self.criterion(logits, y)\n",
        "\n",
        "                if train:\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item() * X.size(0)\n",
        "                preds = logits.argmax(dim=1)\n",
        "                total_correct += (preds == y).sum().item()\n",
        "                total_samples += X.size(0)\n",
        "\n",
        "        avg_loss = total_loss / total_samples\n",
        "        acc = total_correct / total_samples\n",
        "        return avg_loss, acc\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Main training loop.\n",
        "        Tracks and reports both training and validation performance.\n",
        "        \"\"\"\n",
        "        for epoch in range(1, self.num_epochs + 1):\n",
        "            train_loss, train_acc = self._run_one_epoch(self.train_loader, train=True)\n",
        "\n",
        "            if self.val_loader is not None:\n",
        "                val_loss, val_acc = self._run_one_epoch(self.val_loader, train=False)\n",
        "\n",
        "                # Step the LR scheduler based on validation accuracy\n",
        "                if self.scheduler is not None:\n",
        "                    self.scheduler.step(val_acc)\n",
        "\n",
        "                # Track best model\n",
        "                if val_acc > self.best_val_acc:\n",
        "                    self.best_val_acc = val_acc\n",
        "                    self.best_state_dict = {\n",
        "                        k: v.cpu().clone() for k, v in self.model.state_dict().items()\n",
        "                    }\n",
        "\n",
        "                # Log metrics\n",
        "                self.history[\"train_loss\"].append(train_loss)\n",
        "                self.history[\"train_acc\"].append(train_acc)\n",
        "                self.history[\"val_loss\"].append(val_loss)\n",
        "                self.history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "                print(\n",
        "                    f\"Epoch {epoch:02d}/{self.num_epochs} | \"\n",
        "                    f\"Train Loss={train_loss:.4f}, Train Acc={train_acc*100:5.2f}% | \"\n",
        "                    f\"Val Loss={val_loss:.4f}, Val Acc={val_acc*100:5.2f}%\"\n",
        "                )\n",
        "            else:\n",
        "                # No validation set\n",
        "                self.history[\"train_loss\"].append(train_loss)\n",
        "                self.history[\"train_acc\"].append(train_acc)\n",
        "                print(\n",
        "                    f\"Epoch {epoch:02d}/{self.num_epochs} | \"\n",
        "                    f\"Train Loss={train_loss:.4f}, Train Acc={train_acc*100:5.2f}%\"\n",
        "                )\n",
        "\n",
        "        if self.val_loader is not None:\n",
        "            print(f\"\\n[INFO] Best Validation Accuracy = {self.best_val_acc*100:.2f}%\")\n",
        "\n",
        "    def load_best_model(self):\n",
        "        \"\"\"\n",
        "        Restore the best-performing model parameters (based on validation accuracy).\n",
        "        \"\"\"\n",
        "        if self.best_state_dict is not None:\n",
        "            self.model.load_state_dict(self.best_state_dict)\n",
        "            self.model.to(self.device)\n",
        "        else:\n",
        "            print(\"[WARN] No best_state_dict found. Ensure validation was used during training.\")\n",
        "\n",
        "    def test(self, test_loader: DataLoader = None):\n",
        "        \"\"\"\n",
        "        Evaluate the model on the test set.\n",
        "        Automatically loads the best checkpoint if available.\n",
        "        \"\"\"\n",
        "        loader = test_loader or self.test_loader\n",
        "        if loader is None:\n",
        "            raise ValueError(\"No test_loader provided.\")\n",
        "\n",
        "        # Use the best model checkpoint if available\n",
        "        if self.best_state_dict is not None:\n",
        "            self.load_best_model()\n",
        "\n",
        "        test_loss, test_acc = self._run_one_epoch(loader, train=False)\n",
        "        print(f\"[TEST] Loss={test_loss:.4f}, Accuracy={test_acc*100:5.2f}%\")\n",
        "        return test_loss, test_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3da1044f",
      "metadata": {
        "id": "3da1044f"
      },
      "source": [
        "## Train the float model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "fe538867",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe538867",
        "outputId": "9ed9f682-4961-4d20-c4c3-e6befb55eb11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/20 | Train Loss=1.7707, Train Acc=39.73% | Val Loss=1.1984, Val Acc=62.36%\n",
            "Epoch 02/20 | Train Loss=1.1838, Train Acc=59.89% | Val Loss=0.9414, Val Acc=69.33%\n",
            "Epoch 03/20 | Train Loss=0.9982, Train Acc=65.96% | Val Loss=0.8605, Val Acc=71.22%\n",
            "Epoch 04/20 | Train Loss=0.9014, Train Acc=69.27% | Val Loss=0.7646, Val Acc=73.88%\n",
            "Epoch 05/20 | Train Loss=0.8358, Train Acc=71.58% | Val Loss=0.7611, Val Acc=74.64%\n",
            "Epoch 06/20 | Train Loss=0.7842, Train Acc=73.40% | Val Loss=0.7013, Val Acc=76.62%\n",
            "Epoch 07/20 | Train Loss=0.7379, Train Acc=74.59% | Val Loss=0.7044, Val Acc=76.57%\n",
            "Epoch 08/20 | Train Loss=0.7064, Train Acc=75.97% | Val Loss=0.6596, Val Acc=78.33%\n",
            "Epoch 09/20 | Train Loss=0.6799, Train Acc=76.78% | Val Loss=0.6873, Val Acc=77.59%\n",
            "Epoch 10/20 | Train Loss=0.6502, Train Acc=77.77% | Val Loss=0.6607, Val Acc=78.39%\n",
            "Epoch 11/20 | Train Loss=0.6271, Train Acc=78.65% | Val Loss=0.6564, Val Acc=78.44%\n",
            "Epoch 12/20 | Train Loss=0.6026, Train Acc=79.33% | Val Loss=0.6441, Val Acc=78.57%\n",
            "Epoch 13/20 | Train Loss=0.5817, Train Acc=80.28% | Val Loss=0.6528, Val Acc=78.99%\n",
            "Epoch 14/20 | Train Loss=0.5643, Train Acc=80.58% | Val Loss=0.6429, Val Acc=79.35%\n",
            "Epoch 15/20 | Train Loss=0.5557, Train Acc=80.84% | Val Loss=0.6597, Val Acc=78.44%\n",
            "Epoch 16/20 | Train Loss=0.5336, Train Acc=81.65% | Val Loss=0.6511, Val Acc=79.06%\n",
            "Epoch 17/20 | Train Loss=0.5282, Train Acc=82.00% | Val Loss=0.6430, Val Acc=79.21%\n",
            "Epoch 18/20 | Train Loss=0.5062, Train Acc=82.82% | Val Loss=0.6437, Val Acc=79.59%\n",
            "Epoch 19/20 | Train Loss=0.4932, Train Acc=83.14% | Val Loss=0.6633, Val Acc=78.97%\n",
            "Epoch 20/20 | Train Loss=0.4844, Train Acc=83.51% | Val Loss=0.6595, Val Acc=78.93%\n",
            "\n",
            "[INFO] Best Validation Accuracy = 79.59%\n",
            "[TEST] Loss=0.6477, Accuracy=78.38%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6477370482810862, 0.7837505129257284)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model = FloatMLP(num_classes=12, hidden_dim=256, dropout_p=0.3).to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4,\n",
        "    batch_size=256,\n",
        "    num_epochs=20,\n",
        "    scheduler_factor=0.5,\n",
        "    scheduler_patience=3,\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "trainer.train()\n",
        "trainer.test()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30148270",
      "metadata": {
        "id": "30148270"
      },
      "source": [
        "## Train the quantised model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d82cb68b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "d82cb68b",
        "outputId": "db3eab41-27ed-4db7-f8fa-1a1d83d3e214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py:1645: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1939.)\n",
            "  return super().rename(names)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1701101341.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-184726747.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \"\"\"\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-184726747.py\u001b[0m in \u001b[0;36m_run_one_epoch\u001b[0;34m(self, loader, train)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m                             )\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    245\u001b[0m             )\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Lastly, switch back to complex view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = QuantMLPKWS_Dropout(\n",
        "    num_classes=12,\n",
        "    hidden_dim=256,\n",
        "    dropout_p=0.3,\n",
        "    w_bit=3, a_bit=3\n",
        ").to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4,\n",
        "    batch_size=256,\n",
        "    num_epochs=20,\n",
        "    scheduler_factor=0.5,\n",
        "    scheduler_patience=3,\n",
        "    use_input_quant=True,\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "trainer.train()\n",
        "trainer.test()\n",
        "\n",
        "# save weights\n",
        "weight_dir = root_path / \"weights\"\n",
        "weight_dir.mkdir(parents=True, exist_ok=True)\n",
        "torch.save(model.state_dict(), weight_dir / \"mlpw3a3_model_weights.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d6e565",
      "metadata": {
        "id": "04d6e565"
      },
      "source": [
        "## Export your weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3e86d62a",
      "metadata": {
        "id": "3e86d62a"
      },
      "outputs": [],
      "source": [
        "weight_dir = root_path / \"weights\"\n",
        "weight_dir.mkdir(parents=True, exist_ok=True)\n",
        "torch.save(model.state_dict(), weight_dir / \"mlpw3a3_model_weights.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c11f5aa2",
      "metadata": {
        "id": "c11f5aa2"
      },
      "source": [
        "## Now we try 4 bit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "21b326c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "21b326c8",
        "outputId": "113b8faa-e708-49a3-ed0a-f72123452e04"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-853088581.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-184726747.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \"\"\"\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-184726747.py\u001b[0m in \u001b[0;36m_run_one_epoch\u001b[0;34m(self, loader, train)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \"\"\"\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         r\"\"\"\n\u001b[1;32m    298\u001b[0m         \u001b[0mApply\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mused\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mexecuting\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mNode\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = QuantMLPKWS_Dropout(\n",
        "    num_classes=12,\n",
        "    hidden_dim=256,\n",
        "    dropout_p=0.3,\n",
        "    w_bit=4, a_bit=4\n",
        ").to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4,\n",
        "    batch_size=256,\n",
        "    num_epochs=20,\n",
        "    scheduler_factor=0.5,\n",
        "    scheduler_patience=3,\n",
        "    use_input_quant=True,\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "trainer.train()\n",
        "trainer.test()\n",
        "\n",
        "# save weights\n",
        "weight_dir = root_path / \"weights\"\n",
        "weight_dir.mkdir(parents=True, exist_ok=True)\n",
        "torch.save(model.state_dict(), weight_dir / \"mlpw4a4_model_weights.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7596f77d",
      "metadata": {
        "id": "7596f77d"
      },
      "outputs": [],
      "source": [
        "weight_dir = root_path / \"weights\"\n",
        "weight_dir.mkdir(parents=True, exist_ok=True)\n",
        "torch.save(model.state_dict(), weight_dir / \"mlpw4a4_model_weights.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11dbdfb1",
      "metadata": {
        "id": "11dbdfb1"
      },
      "source": [
        "## Export your onnx graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5b0be07c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "5b0be07c",
        "outputId": "a17361b9-8459-4148-b0c1-3afdc5eff20f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "Installation of onnx and onnxoptimizer is required.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3129532347.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     export_qonnx(\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# sometimes use input_t=dummy_input depending on brevitas version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/brevitas/export/__init__.py\u001b[0m in \u001b[0;36mexport_qonnx\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_export_qonnx_dynamo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_export_qonnx_torchscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/brevitas/export/__init__.py\u001b[0m in \u001b[0;36m_export_qonnx_torchscript\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQONNXManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_export_qonnx_torchscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mQONNXManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQONNXDynamoManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/brevitas/export/onnx/manager.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(cls, module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mdisable_warnings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             **onnx_export_kwargs):\n\u001b[0;32m--> 215\u001b[0;31m         return cls.export_onnx(\n\u001b[0m\u001b[1;32m    216\u001b[0m             module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/brevitas/export/onnx/qonnx/manager.py\u001b[0m in \u001b[0;36mexport_onnx\u001b[0;34m(cls, module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0monnx_export_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mQONNX_DOMAIN_STRING\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQONNX_DOMAIN_VERSION\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         return super(QONNXManager, cls).export_onnx(\n\u001b[0m\u001b[1;32m    108\u001b[0m             module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/brevitas/export/onnx/manager.py\u001b[0m in \u001b[0;36mexport_onnx\u001b[0;34m(cls, module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0monnx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Installation of onnx and onnxoptimizer is required.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Export requires one of input_shape, args, or input_t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: Installation of onnx and onnxoptimizer is required.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "from brevitas.export import export_qonnx\n",
        "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
        "from qonnx.core.modelwrapper import ModelWrapper\n",
        "model = QuantMLPKWS_Dropout(\n",
        "    num_classes=12,\n",
        "    hidden_dim=256,\n",
        "    dropout_p=0.3,\n",
        "    w_bit=3,\n",
        "    a_bit=3\n",
        ").cpu()\n",
        "\n",
        "# --- 1. Load trained weights ---\n",
        "weight_dir = root_path / \"weights\"\n",
        "state_dict = torch.load(weight_dir / \"mlpw3a3_model_weights.pth\", map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()  # Always switch to eval mode before export\n",
        "\n",
        "# --- 2. Prepare dummy input ---\n",
        "# The dummy input shape must match the models expected input (B, 1, 10, 49)\n",
        "dummy_input = torch.randn(1, 1, 10, 49)\n",
        "\n",
        "# --- 3. Export to QONNX (for FINN / FPGA deployment) ---\n",
        "export_path = str(root_path / \"exports\" / \"kws_mlp_w3a3_qonnx.onnx\")\n",
        "\n",
        "# Ensure the export directory exists\n",
        "export_dir = Path(export_path).parent\n",
        "export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    export_qonnx(\n",
        "        model,\n",
        "        args=dummy_input,  # sometimes use input_t=dummy_input depending on brevitas version\n",
        "        export_path=export_path\n",
        "    )\n",
        "\n",
        "# clean-up\n",
        "qonnx_cleanup(export_path, out_file=export_path)\n",
        "\n",
        "# Setting the input datatype explicitly because it doesn't get derived from the export function\n",
        "model = ModelWrapper(export_path)\n",
        "model.set_tensor_datatype(model.graph.input[0].name, DataType[\"INT8\"])\n",
        "model.save(export_path)\n",
        "\n",
        "print(\"QONNX model successfully exported to:\", export_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7800e50",
      "metadata": {
        "id": "d7800e50"
      },
      "source": [
        "## Export 4 bit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde090b7",
      "metadata": {
        "id": "bde090b7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "from brevitas.export import export_qonnx\n",
        "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
        "from qonnx.core.modelwrapper import ModelWrapper\n",
        "model = QuantMLPKWS_Dropout(\n",
        "    num_classes=12,\n",
        "    hidden_dim=256,\n",
        "    dropout_p=0.3,\n",
        "    w_bit=4,\n",
        "    a_bit=4\n",
        ").cpu()\n",
        "\n",
        "# --- 1. Load trained weights ---\n",
        "weight_dir = root_path / \"weights\"\n",
        "state_dict = torch.load(weight_dir / \"mlpw4a4_model_weights.pth\", map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()  # Always switch to eval mode before export\n",
        "\n",
        "# --- 2. Prepare dummy input ---\n",
        "# The dummy input shape must match the models expected input (B, 1, 10, 49)\n",
        "dummy_input = torch.randn(1, 1, 10, 49)\n",
        "\n",
        "# --- 3. Export to QONNX (for FINN / FPGA deployment) ---\n",
        "export_path = str(root_path / \"exports\" / \"kws_mlp_w4a4_qonnx.onnx\")\n",
        "\n",
        "# Ensure the export directory exists\n",
        "export_dir = Path(export_path).parent\n",
        "export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    export_qonnx(\n",
        "        model,\n",
        "        args=dummy_input,  # sometimes use input_t=dummy_input depending on brevitas version\n",
        "        export_path=export_path\n",
        "    )\n",
        "\n",
        "# clean-up\n",
        "qonnx_cleanup(export_path, out_file=export_path)\n",
        "\n",
        "# Setting the input datatype explicitly because it doesn't get derived from the export function\n",
        "model = ModelWrapper(export_path)\n",
        "model.set_tensor_datatype(model.graph.input[0].name, DataType[\"INT8\"])\n",
        "model.save(export_path)\n",
        "\n",
        "print(\"QONNX model successfully exported to:\", export_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7dbdc14",
      "metadata": {
        "id": "f7dbdc14"
      },
      "source": [
        "## Try other bit width and answer:\n",
        "- Q1: What is the bitwidth of float DNN model\n",
        "- Q2: What are the difference between the float and the quantised model?\n",
        "- Q3: Try different weight and activation bit width, what did you find? Weight bw and activation bw which is more important?\n",
        "- Q4: What is the accuracy - bw trade-off here? In practice, how to make the decision?\n",
        "- Q5(optional): Fine tune the hyper parameters, can you break the accuracy - bw edge (illustrate with a Acc-bw curve)?\n",
        "- Q6(optional): Considering other model compression strategies, can you further break the accuracy - bw edge  (illustrate with a Acc-bw curve)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d00e39",
      "metadata": {
        "id": "60d00e39"
      },
      "outputs": [],
      "source": [
        "your_weight_bitwidth = 4  # Example: change to 4 bits\n",
        "your_activation_bitwidth = 2  # Example: change to 4 bits\n",
        "\n",
        "\n",
        "model = QuantMLPKWS_Dropout(\n",
        "    num_classes=12,\n",
        "    hidden_dim=256,\n",
        "    dropout_p=0.3,\n",
        "    w_bit=your_weight_bitwidth, a_bit=your_activation_bitwidth\n",
        ").to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4,\n",
        "    batch_size=256,\n",
        "    num_epochs=20,\n",
        "    scheduler_factor=0.5,\n",
        "    scheduler_patience=3,\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "# trainer.train()\n",
        "# trainer.test()\n",
        "# Then use same code below to export the model\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lab5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}